{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq_NJPwK0sj8"
      },
      "source": [
        "## Recurrent Neural Networks: Shakespeare task\n",
        "\n",
        "Bronwyn Bowles-King\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This project will produce a model called fake_shake, which attempts to mimic the dramatic writing style of Shakespeare from a sample of his original plays. It relies on a Recurrent Neural Network (RNN) created in Python with TensorFlow.\n",
        "\n",
        "### 0. Preparation steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbDrwjen0t3K",
        "outputId": "3494890c-afde-45e8-802a-12e61fc0ee11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text:  1115394\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import requests\n",
        "import random\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Import Shakespeare dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "print(\"Length of text: \", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUjvSo_ni4ac"
      },
      "source": [
        "### 1. Define a function to generate text from the trained RNN\n",
        "\n",
        "The most critical part of the function defined below to note is the for loop (for i in range (length)) where the model predicts the next character and appends it to the seed sequence it started with (seed_text). The model needs a prompt to begin with from Shakespeare's original works used to train the model.\n",
        "\n",
        "The function works in iterations with a current sequence of characters (the 'window') that it prepares to feed to the model in the required format (char_to_int). The trained model is a type of Long Short-Term Memory (LSTM) model that predicts the probability of the next likely character.\n",
        "\n",
        "The model does not generate whole words at a time, just one character. The line np.argmax(prediction) chooses the character with the highest predicted probability. This predicted character is translated back from its index to its actual character (int_to_char) and the window is updated with this new character. This process repeats until the required number of characters is generated.\n",
        "\n",
        "What is special about RNNs such as this compared to feedforward networks is that it always bases its predictions on previously generated sequences, thus having a type of 'memory' or context-awareness that makes it seem convincingly intelligent. However, like AI we can currently use, it is based on mathematical and computational functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jbllax25i1vL"
      },
      "outputs": [],
      "source": [
        "def fake_shake(model, seed_text, char_to_int, int_to_char, length=2000):\n",
        "\n",
        "    # Prepare initial pattern from seed_text\n",
        "    seq_length = len(seed_text)\n",
        "    pattern = [char_to_int[char] for char in seed_text]\n",
        "    generated = []\n",
        "\n",
        "    for i in range(length):\n",
        "        # Prepare input of shape (1, seq_length, 1)\n",
        "        x = np.reshape(pattern, (1, seq_length, 1))\n",
        "        x = x / float(len(char_to_int))\n",
        "\n",
        "        # Model to predict the next character\n",
        "        prediction = model.predict(x, verbose=0)\n",
        "        index = np.argmax(prediction)\n",
        "        result = int_to_char[index]\n",
        "\n",
        "        # Append result and update the pattern\n",
        "        generated.append(result)\n",
        "        pattern.append(index)\n",
        "        # Move to the next window\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "    return seed_text + ''.join(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7TXNckECL2i"
      },
      "source": [
        "### 2. Pre-process text for encoding\n",
        "\n",
        "The characters in the Shakespeare sample are mapped with a dictionary to integers that will represent them. The program will then create overlapping sequences of the set length (seq_length) that work across the encoded text to train the model to predict the next character (seq_out) when given an input sequence (seq_in)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c4b4ed3",
        "outputId": "cf67b480-15c7-495a-d2d4-9dbbfa080880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: 65\n",
            "Number of sequences: 1115294\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(f'Unique characters: {len(chars)}')\n",
        "\n",
        "# Map characters to unique integers and back again\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encode text as integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]\n",
        "\n",
        "# Number of input sequences\n",
        "seq_length = 100\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Iterate through encoded text to create sequences and target data\n",
        "for i in range(0, len(encoded_text) - seq_length):\n",
        "    seq_in = encoded_text[i:i + seq_length]\n",
        "    seq_out = encoded_text[i + seq_length]\n",
        "    X.append(seq_in)\n",
        "    y.append(seq_out)\n",
        "\n",
        "print(f'Number of sequences: {len(X)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5ac222"
      },
      "source": [
        "Next, the input data (X) is reshaped and normalised to fit the requirements of the training and model. The data needs to be a 3D tensor with sample, time step, and features dimensions. The samples are the number of independent sequences in the dataset. 100 characters is one sample in this model. Time steps are the length of a sequence. There are 100 time steps per sample because there are sequences of 100 characters at a time. The features dimension is the number of features at each time step. One-hot encoding is applied for the target variable (y)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40623ea7",
        "outputId": "7bf5d149-1ebb-4035-f328-b48627513af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_reshaped shape: (1115294, 100, 1)\n",
            "y_categorical shape: (1115294, 65)\n"
          ]
        }
      ],
      "source": [
        "# Reshape X to 3D tensor [samples, time steps, features]\n",
        "X_reshaped = np.reshape(X, (len(X), seq_length, 1))\n",
        "\n",
        "# Normalise X data\n",
        "X_normalized = X_reshaped / float(len(chars))\n",
        "\n",
        "# One-hot encode y (output) variable\n",
        "y_categorical = to_categorical(y, num_classes=len(chars))\n",
        "\n",
        "print(\"X_reshaped shape:\", X_reshaped.shape)\n",
        "print(\"y_categorical shape:\", y_categorical.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14f947a"
      },
      "source": [
        "### 3. Define and compile the model\n",
        "\n",
        "The code below now defines the simple RNN model for character-level text generation using an LSTM layer with 256 hidden units or 'neurons'. The LSTM is suited to learning sequences in steps which is best for text data.\n",
        "\n",
        "The input_shape argument tells the model the expected shape of the input data. A fully connected (Dense) layer is added with the same number of unique characters (65) in the text dataset from Shakespeare's works, including upper and lowercase characters, numbers, punctuation, etc.\n",
        "\n",
        "The activation function applied is the softmax one to turn the output into a probability distribution over all these possible characters. The model will thus predict across the probability of each of these 65 characters which one is most likely to go next.\n",
        "\n",
        "The last line of code below prepares the model for training by specifying how it should learn (Adam optimiser), how mistakes are quantified (categorical cross‑entropy loss), and how to monitor progress (accuracy score).\n",
        "\n",
        "The loss function is categorical cross‑entropy for multi‑class classification problem such as predicting the next character out of 65 possiblities. The model uses this function to measure the difference between its predictions and the actual character in training examples. Over training, the difference will be minimised.\n",
        "\n",
        "Adaptive Moment Estimation (Adam) is an optimisation algorithm applied here that updates the model's weights and then minimises the loss function over time. It changes the learning rate during training to help the model improve more quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cd1852ae"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(X_normalized.shape[1], X_normalized.shape[2])))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a38bd77"
      },
      "source": [
        "### 4. Training routine\n",
        "\n",
        "The model is now trained for 10 rounds (epochs) - as per the instructions for this task, although this number of rounds is too few to train the model properly. It was found that changing to the GPU runtime type in Google Colab sped up the process. This reduced training time from over 10 hours, which did not complete, to a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b66b80a",
        "outputId": "b6f7546b-364a-461a-e988-72ff663a0f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 14ms/step - accuracy: 0.1973 - loss: 2.9543\n",
            "Epoch 2/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 15ms/step - accuracy: 0.2756 - loss: 2.5696\n",
            "Epoch 3/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 15ms/step - accuracy: 0.3061 - loss: 2.4324\n",
            "Epoch 4/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 14ms/step - accuracy: 0.3232 - loss: 2.3504\n",
            "Epoch 5/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 14ms/step - accuracy: 0.3378 - loss: 2.2902\n",
            "Epoch 6/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.3518 - loss: 2.2391\n",
            "Epoch 7/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 14ms/step - accuracy: 0.3664 - loss: 2.1914\n",
            "Epoch 8/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 14ms/step - accuracy: 0.3772 - loss: 2.1548\n",
            "Epoch 9/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 14ms/step - accuracy: 0.3867 - loss: 2.1186\n",
            "Epoch 10/10\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.3936 - loss: 2.0925\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_normalized, y_categorical, epochs=10, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g-465KMhZZT"
      },
      "source": [
        "### 5. Generate synthetic Shakespearean dialogue\n",
        "\n",
        "Using the fake_shake function previously defined (section 1), we can now request the trained RNN model to generate text. The model needs seed text as a starting point or prompt and then it generates words until the target character count is reached.\n",
        "\n",
        "In this case, I requested 1 000 characters to ensure it reached the minimum word count of 100. The model does not perform very well and this was expected. We can see why in the output from the previous cell as the model only achieved an accuracy of 39% over 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5RcUFNphXuv",
        "outputId": "ba4aaee4-f0e1-4348-8c51-6b19ca4f33d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'aged custom,\n",
            "But by your voices, will not so permit me;\n",
            "Your voices therefore.' When we granted thae,\n",
            "Io the pooe seat thet whth the paaee of thee,\n",
            "And then the world the world the whrl the world,\n",
            "And then the world the world shel then the world,\n",
            "And then the world the world shell toeak the soateh\n",
            "That thet wh lave to the mor the poieters shat\n",
            "That the whsl sooe thet whth the paaee of thee,\n",
            "And then the world the world shel then the world,\n",
            "And then the world the world shel then the world,\n",
            "And then the world the world shell toeak the soateh\n",
            "That thet wh lave to the mor the poieters shat\n",
            "That the whsl sooe thet whth the paaee of thee,\n",
            "And then the world the world shel then the world,\n",
            "And then the world the world shel then the world,\n",
            "And then the world the world shell toeak the soateh\n",
            "That thet wh lave to the mor the poieters shat\n",
            "That the whsl sooe thet whth the paaee of thee,\n",
            "And then the world the world shel then the world,\n",
            "And then the world the world shel then the world,\n",
            "And then the world the world shell toeak the soateh\n",
            "That thet wh lave to the mor the poieters shat\n",
            "That the whs\n"
          ]
        }
      ],
      "source": [
        "# Select seed_text from encoded_text as starting point for the model\n",
        "start = random.randint(0, len(encoded_text) - seq_length - 1)\n",
        "seed_text = ''.join([int_to_char[i] for i in encoded_text[start:start+seq_length]])\n",
        "\n",
        "# Run the function\n",
        "monologue = fake_shake(model, seed_text, char_to_int, int_to_char, length=1000)\n",
        "print(monologue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9QOOE77tsOw"
      },
      "source": [
        "### 6. Retrain the model for 30 epochs and generate text again\n",
        "\n",
        "The model is now retrained for 30 epochs to see if it will perform better. The model has improved as there is less repetition, but can be improved further (Trekhleb, 2020). The model is not complex enough to create more convincing text in the style of Shakespeare. However, this exercise has demonstrated what is possible with an RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxvmOugTsK1B",
        "outputId": "1e8a2abf-d6da-4e47-cb82-26b6520a18e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 14ms/step - accuracy: 0.4006 - loss: 2.0668\n",
            "Epoch 2/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4063 - loss: 2.0468\n",
            "Epoch 3/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4133 - loss: 2.0232\n",
            "Epoch 4/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.3947 - loss: 2.1053\n",
            "Epoch 5/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4195 - loss: 1.9962\n",
            "Epoch 6/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4253 - loss: 1.9769\n",
            "Epoch 7/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4313 - loss: 1.9586\n",
            "Epoch 8/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4319 - loss: 1.9498\n",
            "Epoch 9/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 14ms/step - accuracy: 0.4350 - loss: 1.9404\n",
            "Epoch 10/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 14ms/step - accuracy: 0.4262 - loss: 1.9830\n",
            "Epoch 11/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4368 - loss: 1.9337\n",
            "Epoch 12/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4410 - loss: 1.9194\n",
            "Epoch 13/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4444 - loss: 1.9042\n",
            "Epoch 14/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4459 - loss: 1.8987\n",
            "Epoch 15/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 14ms/step - accuracy: 0.4506 - loss: 1.8836\n",
            "Epoch 16/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4522 - loss: 1.8766\n",
            "Epoch 17/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4550 - loss: 1.8666\n",
            "Epoch 18/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4563 - loss: 1.8616\n",
            "Epoch 19/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4589 - loss: 1.8530\n",
            "Epoch 20/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4374 - loss: 1.9494\n",
            "Epoch 21/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4460 - loss: 1.9046\n",
            "Epoch 22/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4559 - loss: 1.8673\n",
            "Epoch 23/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4615 - loss: 1.8468\n",
            "Epoch 24/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4642 - loss: 1.8366\n",
            "Epoch 25/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4675 - loss: 1.8218\n",
            "Epoch 26/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4694 - loss: 1.8131\n",
            "Epoch 27/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 14ms/step - accuracy: 0.4721 - loss: 1.8053\n",
            "Epoch 28/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4748 - loss: 1.7949\n",
            "Epoch 29/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14ms/step - accuracy: 0.4762 - loss: 1.7901\n",
            "Epoch 30/30\n",
            "\u001b[1m8714/8714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14ms/step - accuracy: 0.4774 - loss: 1.7813\n",
            "'aged custom,\n",
            "But by your voices, will not so permit me;\n",
            "Your voices therefore.' When we granted thae theneende,\n",
            "That she had sheledr the common thenrelves,\n",
            "That the best rrend as hen to hev the sear,\n",
            "And the domd coosens of the marter of Yil,\n",
            "And then the soatehter of the world thenrelves,\n",
            "That they do soeak the coomo of the marter of Yil\n",
            "The saret of the dour that the courte of his\n",
            "The whrte man shall be to desire the sear,\n",
            "And then the soo mf the mank of the marter,\n",
            "And the domd coonent of the marter shates,\n",
            "And the domd coosent of the mane of hard,\n",
            "And the devtir that the common dead the sea\n",
            "The whrse shall be aooe to be a mine of menct,\n",
            "The sane ie say the couste of his toeelhng senves\n",
            "That the peeprr that the courte of his crosner sear\n",
            "The whrte man shall be the dour that the paie,\n",
            "The sane ie say the couste of hiahe of hiahe\n",
            "That the peeprr of the manes of his crotners,\n",
            "And the devtlren of the house of Yolk\n",
            "Then the mentters of the soatehter of the siahe\n",
            "That the best rrend as her ar many hase,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the mane of his crotn,\n",
            "And the devtir of the sear of his doom,\n",
            "The sane ae holest for the mane of hare,\n",
            "And the devtir then ont the coomo of the Thr\n",
            "Ofer the hand shall be a mine of harmy doun\n",
            "The whrle that the courte of his brother siale\n",
            "The shale bod the paie of the mane of hare,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "And the devtir of the sear of his dromn,\n",
            "And the devtir of the sear of his crown,\n",
            "An\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_normalized, y_categorical, epochs=30, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsV2TwkxwmEU"
      },
      "source": [
        "**References**\n",
        "\n",
        "Geeks4Geeks. (2025). ML | ADAM (Adaptive Moment Estimation) Optimization. https://www.geeksforgeeks.org/machine-learning/adam-adaptive-moment-estimation-optimization-ml\n",
        "\n",
        "HyperionDev. (2025). Build a Neural Network. Course materials. Private repository, GitHub.\n",
        "\n",
        "HyperionDev. (2025). Neural Networks. Course materials. Private repository, GitHub.\n",
        "\n",
        "HyperionDev. (2025). Recurrent Neural Networks. Course materials. Private repository, GitHub.\n",
        "\n",
        "Karpathy, A. (2016). Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy. GitHub. https://gist.github.com/karpathy/d4dee566867f8291f086\n",
        "\n",
        "Kithmanthie, R. (2025). Predicting the Next Character with RNN: A Simple Introduction Using Shakespeare's Text. Medium. https://medium.com/@ritharaedirisinghe/predicting-the-next-character-with-rnn-a-simple-introduction-using-shakespeares-text-88e62550ac17\n",
        "\n",
        "PyTorch. (2024). Running Tutorials in Google Colab. https://docs.pytorch.org/tutorials/beginner/colab.html\n",
        "\n",
        "TensorFlow. (n.d.). Text generation with an RNN.\n",
        "https://www.tensorflow.org/text/tutorials/text_generation\n",
        "\n",
        "TensorFlow. (2023). Keras: The high-level API for TensorFlow. https://www.tensorflow.org/guide/keras\n",
        "\n",
        "Trekhleb, O. (2020). Shakespeare Text Generation (using RNN LSTM). Google Colab. https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

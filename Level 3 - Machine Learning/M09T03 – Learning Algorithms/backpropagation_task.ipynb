{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfAOLGB2IfjV"
   },
   "source": [
    "## Backpropagation task\n",
    "\n",
    "Bronwyn Bowles-King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1205,
     "status": "ok",
     "timestamp": 1733132797420,
     "user": {
      "displayName": "Pierre Roodman",
      "userId": "08657076676700421712"
     },
     "user_tz": -120
    },
    "id": "tua4cG6oJQuo"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "# Tanh activation function and its derivative\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 1511,
     "status": "ok",
     "timestamp": 1733132802301,
     "user": {
      "displayName": "Pierre Roodman",
      "userId": "08657076676700421712"
     },
     "user_tz": -120
    },
    "id": "QSwVcf9VJSb-",
    "outputId": "109bae29-dfb0-4ec5-92ea-b37efe49f6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.50183952]\n",
      " [ 1.80285723]\n",
      " [ 0.92797577]\n",
      " [ 0.39463394]\n",
      " [-1.37592544]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X = np.random.uniform(-2, 2, size=(1000, 1))  # Creates array X of 1000 random values in range [-2, 2].\n",
    "y = (X >= 0).astype(int)  # Binary classification label added for X non-negative (class 1) or negative (class 0).\n",
    "\n",
    "# Preview data\n",
    "print(X[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model parameters\n",
    "input_size = 1\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Training parameters\n",
    "num_iter = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4l-2z7rPJwO1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6931\n",
      "Iteration 100, Loss: 0.6931\n",
      "Iteration 200, Loss: 0.6931\n",
      "Iteration 300, Loss: 0.6931\n",
      "Iteration 400, Loss: 0.6931\n",
      "Iteration 500, Loss: 0.6931\n",
      "Iteration 600, Loss: 0.6931\n",
      "Iteration 700, Loss: 0.6931\n",
      "Iteration 800, Loss: 0.6930\n",
      "Iteration 900, Loss: 0.6930\n"
     ]
    }
   ],
   "source": [
    "# Training loop on 75% of the data\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iter in range(num_iter):\n",
    "    # Forward pass \n",
    "    Z1 = np.dot(X_train, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    y_pred = sigmoid(Z2)\n",
    "\n",
    "    # Compute loss with binary cross-entropy (BCE)\n",
    "    loss = -np.mean(y_train * np.log(y_pred + 1e-8) + (1 - y_train) * np.log(1 - y_pred + 1e-8))\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Backward pass for BCE + sigmoid\n",
    "    dZ2 = (y_pred - y_train) / len(y_train) \n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * sigmoid_derivative(Z1)\n",
    "    dW1 = np.dot(X_train.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update weights and biases\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f'Iteration {iter}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6936\n",
      "Test accuracy: 47.60%\n",
      "\n",
      "Predictions: [1 1 1 1 1 1 1 1 1 1 1]\n",
      "Actual: [0 1 0 0 1 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on 25% test set\n",
    "\n",
    "Z1_test = np.dot(X_test, W1) + b1\n",
    "A1_test = sigmoid(Z1_test)\n",
    "Z2_test = np.dot(A1_test, W2) + b2\n",
    "y_pred_test = sigmoid(Z2_test)\n",
    "\n",
    "y_pred_labels = (y_pred_test >= 0.5).astype(int)\n",
    "\n",
    "# Calculate loss on test set\n",
    "test_loss = -np.mean(y_test * np.log(y_pred_test + 1e-8) + (1 - y_test) * np.log(1 - y_pred_test + 1e-8))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Display the first 10 predictions and actual values\n",
    "print(\"\\nPredictions:\", y_pred_labels.flatten()[:11])\n",
    "print(\"Actual:\", y_test.flatten()[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1brrDZDBK2Yn"
   },
   "source": [
    "After running this baseline test, we can see the neural network has performed poorly in predicting class 0 (negative values) as well as in class 1 (positive values). It predicts all classes as 1 or positive values, meaning that it cannot really distinguish between 0 or 1 properly. The accuracy score (47.6%) thus remains around 50%. \n",
    "\n",
    "The model above applied the sigmoid function for the forward pass and its derivative for the backwards pass. The parameters were an input size of 1 unit or neuron, hidden layer of 4, and output size of 1. The weights (W1 and W2) are randomised (random.randn) small numbers (0.01) based on the size of the input, hidden and output layers. For the training aspect, the parameters are 1 000 epochs or training loops with a learning rate of 0.01.\n",
    "\n",
    "We now test the impact of different values for the learning rate and number of iterations. So we will do a few tests below to see how changing one parameter at a time affects the outcome. All the other parameters will be kept the same as the baseline test above. \n",
    "\n",
    "**Test with different numbers of epochs**\n",
    "\n",
    "The MLP training was run again using the same process as before. The results when doubling the number of training epochs to 2 000, as shown below, were the same as the baseline of 47.6% accuracy. For the test using half as many epochs as the baseline (500), the results were again the same (47.6%), but this is not shown below. This indicates that the number of training runs are not the main problem here, at least not on their own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6931\n",
      "Iteration 100, Loss: 0.6931\n",
      "Iteration 200, Loss: 0.6931\n",
      "Iteration 300, Loss: 0.6931\n",
      "Iteration 400, Loss: 0.6930\n",
      "Iteration 500, Loss: 0.6930\n",
      "Iteration 600, Loss: 0.6930\n",
      "Iteration 700, Loss: 0.6929\n",
      "Iteration 800, Loss: 0.6929\n",
      "Iteration 900, Loss: 0.6928\n",
      "Iteration 1000, Loss: 0.6927\n",
      "Iteration 1100, Loss: 0.6926\n",
      "Iteration 1200, Loss: 0.6925\n",
      "Iteration 1300, Loss: 0.6923\n",
      "Iteration 1400, Loss: 0.6920\n",
      "Iteration 1500, Loss: 0.6917\n",
      "Iteration 1600, Loss: 0.6913\n",
      "Iteration 1700, Loss: 0.6908\n",
      "Iteration 1800, Loss: 0.6902\n",
      "Iteration 1900, Loss: 0.6893\n",
      "Test loss: 0.6888\n",
      "Test accuracy: 61.20%\n"
     ]
    }
   ],
   "source": [
    "# Initialise model parameters\n",
    "input_size = 1\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Training parameters\n",
    "num_iter = 2000  # Increase epochs x 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training loop on 75% of the data\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iter in range(num_iter):\n",
    "    # Forward pass \n",
    "    Z1 = np.dot(X_train, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    y_pred = sigmoid(Z2)\n",
    "\n",
    "    # Compute loss with binary cross-entropy (BCE)\n",
    "    loss = -np.mean(y_train * np.log(y_pred + 1e-8) + (1 - y_train) * np.log(1 - y_pred + 1e-8))\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Backward pass for BCE + sigmoid\n",
    "    dZ2 = (y_pred - y_train) / len(y_train) \n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * sigmoid_derivative(Z1)\n",
    "    dW1 = np.dot(X_train.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update weights and biases\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f'Iteration {iter}, Loss: {loss:.4f}')\n",
    "\n",
    "# Make predictions on the 25% test set\n",
    "\n",
    "Z1_test = np.dot(X_test, W1) + b1\n",
    "A1_test = sigmoid(Z1_test)\n",
    "Z2_test = np.dot(A1_test, W2) + b2\n",
    "y_pred_test = sigmoid(Z2_test)\n",
    "\n",
    "y_pred_labels = (y_pred_test >= 0.5).astype(int)\n",
    "\n",
    "# Calculate loss on test set\n",
    "test_loss = -np.mean(y_test * np.log(y_pred_test + 1e-8) + (1 - y_test) * np.log(1 - y_pred_test + 1e-8))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test with different learning rates**\n",
    "\n",
    "We will now try a slower learning rate of 0.001 and a faster rate of 0.1. The training runs will return to 1 000. Instead of improving, the neura network continues to predict all classes as positive numbers with a slower learning rate of 0.001. However, a faster learning rate, as shown below, helps as the accuracy is now very high at 98.8%. \n",
    "\n",
    "The network thus benefits from a faster rate as it is able to find a good solution more quickly, but loses accuracy when training is too slow. This may be because the dataset is fairly simple. The learning rate of 0.1 is not necessarily the best one for this network when it comes to unseen data as other factors can affect the real performance of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6932\n",
      "Iteration 100, Loss: 0.6931\n",
      "Iteration 200, Loss: 0.6925\n",
      "Iteration 300, Loss: 0.6864\n",
      "Iteration 400, Loss: 0.6333\n",
      "Iteration 500, Loss: 0.4581\n",
      "Iteration 600, Loss: 0.2952\n",
      "Iteration 700, Loss: 0.2075\n",
      "Iteration 800, Loss: 0.1598\n",
      "Iteration 900, Loss: 0.1311\n",
      "Test loss: 0.1188\n",
      "Test accuracy: 98.80%\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "num_iter = 1000\n",
    "learning_rate = 0.1  # Faster learning rate\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iter in range(num_iter):\n",
    "    Z1 = np.dot(X_train, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    y_pred = sigmoid(Z2)\n",
    "\n",
    "    loss = -np.mean(y_train * np.log(y_pred + 1e-8) + (1 - y_train) * np.log(1 - y_pred + 1e-8))\n",
    "    losses.append(loss)\n",
    "\n",
    "    dZ2 = (y_pred - y_train) / len(y_train) \n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * sigmoid_derivative(Z1)\n",
    "    dW1 = np.dot(X_train.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f'Iteration {iter}, Loss: {loss:.4f}')\n",
    "\n",
    "Z1_test = np.dot(X_test, W1) + b1\n",
    "A1_test = sigmoid(Z1_test)\n",
    "Z2_test = np.dot(A1_test, W2) + b2\n",
    "y_pred_test = sigmoid(Z2_test)\n",
    "\n",
    "y_pred_labels = (y_pred_test >= 0.5).astype(int)\n",
    "\n",
    "test_loss = -np.mean(y_test * np.log(y_pred_test + 1e-8) + (1 - y_test) * np.log(1 - y_pred_test + 1e-8))\n",
    "\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test with different numbers of nodes**\n",
    "\n",
    "The learning rate is returned to 0.01 as is the case for the baseline network. When different numbers of nodes are used for the hidden layer, whether more or fewer than 4, the network does not necessarily perform better, if at all. \n",
    "\n",
    "With twice as many (8) nodes, the score is still low at 47.6%, and the same as the previous baseline test, as shown below. Increasing the number of nodes even further does not improve the accuracy above around 50%. With half as many nodes as the baseline (2), the accuracy score is slightly better but not encouraging at 51.2% (not shown below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6931\n",
      "Iteration 100, Loss: 0.6931\n",
      "Iteration 200, Loss: 0.6931\n",
      "Iteration 300, Loss: 0.6931\n",
      "Iteration 400, Loss: 0.6930\n",
      "Iteration 500, Loss: 0.6930\n",
      "Iteration 600, Loss: 0.6930\n",
      "Iteration 700, Loss: 0.6929\n",
      "Iteration 800, Loss: 0.6929\n",
      "Iteration 900, Loss: 0.6928\n",
      "Test loss: 0.6933\n",
      "Test accuracy: 47.60%\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "hidden_size = 8  # Adjusted nodes\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "num_iter = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iter in range(num_iter):\n",
    "    Z1 = np.dot(X_train, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    y_pred = sigmoid(Z2)\n",
    "\n",
    "    loss = -np.mean(y_train * np.log(y_pred + 1e-8) + (1 - y_train) * np.log(1 - y_pred + 1e-8))\n",
    "    losses.append(loss)\n",
    "\n",
    "    dZ2 = (y_pred - y_train) / len(y_train) \n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * sigmoid_derivative(Z1)\n",
    "    dW1 = np.dot(X_train.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f'Iteration {iter}, Loss: {loss:.4f}')\n",
    "\n",
    "Z1_test = np.dot(X_test, W1) + b1\n",
    "A1_test = sigmoid(Z1_test)\n",
    "Z2_test = np.dot(A1_test, W2) + b2\n",
    "y_pred_test = sigmoid(Z2_test)\n",
    "\n",
    "y_pred_labels = (y_pred_test >= 0.5).astype(int)\n",
    "\n",
    "test_loss = -np.mean(y_test * np.log(y_pred_test + 1e-8) + (1 - y_test) * np.log(1 - y_pred_test + 1e-8))\n",
    "\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the activation function in the hidden layer** \n",
    "\n",
    "The hyperbolic tangent (tanh) function is an alternative to the sigmoid function. It is similar to the sigmoid, but gives output values between -1 and 1, and it has a simple derivative calculation (Geeks4Geeks, 2025; Khan, 2024). The tanh function is tested for the hidden layer, but the outer layer still uses the sigmoid function. All other parameters are returned to the baseline. \n",
    "\n",
    "The network performs surprisingly well, showing an accuracy of 98.8%, despite the slower learning rate that has so far not been successful (0.01). The network thus benefits from combining two different activation functions, sigmoid and tanh. Alternatively, a faster learning rate can be used for the network. In the last code chunk, I will combine both the faster learning rate and use tanh in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6928\n",
      "Iteration 100, Loss: 0.6923\n",
      "Iteration 200, Loss: 0.6907\n",
      "Iteration 300, Loss: 0.6866\n",
      "Iteration 400, Loss: 0.6758\n",
      "Iteration 500, Loss: 0.6501\n",
      "Iteration 600, Loss: 0.5983\n",
      "Iteration 700, Loss: 0.5191\n",
      "Iteration 800, Loss: 0.4293\n",
      "Iteration 900, Loss: 0.3490\n",
      "Test loss: 0.2940\n",
      "Test accuracy: 98.80%\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "num_iter = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iter in range(num_iter):\n",
    "    Z1 = np.dot(X_train, W1) + b1\n",
    "    A1 = tanh(Z1)  # tanh for hidden layers\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    y_pred = sigmoid(Z2)  # sigmoid for output\n",
    "\n",
    "    # Clip predictions to ensure values are kept in a stable range\n",
    "    y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    loss = -np.mean(y_train * np.log(y_pred) + (1 - y_train) * np.log(1 - y_pred))\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Backward pass for BCE + tanh\n",
    "    dZ2 = (y_pred - y_train) / len(y_train) \n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * tanh_derivative(A1)  \n",
    "    dW1 = np.dot(X_train.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f'Iteration {iter}, Loss: {loss:.4f}')\n",
    "\n",
    "Z1_test = np.dot(X_test, W1) + b1\n",
    "A1_test = tanh(Z1_test)\n",
    "Z2_test = np.dot(A1_test, W2) + b2\n",
    "y_pred_test = sigmoid(Z2_test)\n",
    "\n",
    "y_pred_labels = (y_pred_test >= 0.5).astype(int)\n",
    "\n",
    "test_loss = -np.mean(y_test * np.log(y_pred_test + 1e-8) + (1 - y_test) * np.log(1 - y_pred_test + 1e-8))\n",
    "\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Faster learning rate and tanh activation function in the hidden layer** \n",
    "\n",
    "Combining the faster learning rate of 0.1 and applying tanh as the activation function in the hidden layer has now achieved 99.6% accuracy for the network. This is not a major improvement over using only one of these at a time, but still shows that the two parameters work well for this small network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6931\n",
      "Iteration 100, Loss: 0.3803\n",
      "Iteration 200, Loss: 0.1064\n",
      "Iteration 300, Loss: 0.0694\n",
      "Iteration 400, Loss: 0.0548\n",
      "Iteration 500, Loss: 0.0467\n",
      "Iteration 600, Loss: 0.0415\n",
      "Iteration 700, Loss: 0.0377\n",
      "Iteration 800, Loss: 0.0349\n",
      "Iteration 900, Loss: 0.0327\n",
      "Test loss: 0.0352\n",
      "Test accuracy: 99.60%\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "num_iter = 1000\n",
    "learning_rate = 0.1  # Faster learning rate\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iter in range(num_iter):\n",
    "    Z1 = np.dot(X_train, W1) + b1\n",
    "    A1 = tanh(Z1)  # tanh for hidden layers\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    y_pred = sigmoid(Z2)  # sigmoid for output\n",
    "\n",
    "    # Clip predictions to ensure values are kept in a stable range\n",
    "    y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    loss = -np.mean(y_train * np.log(y_pred) + (1 - y_train) * np.log(1 - y_pred))\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Backward pass for BCE + tanh\n",
    "    dZ2 = (y_pred - y_train) / len(y_train) \n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * tanh_derivative(A1)  \n",
    "    dW1 = np.dot(X_train.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f'Iteration {iter}, Loss: {loss:.4f}')\n",
    "\n",
    "Z1_test = np.dot(X_test, W1) + b1\n",
    "A1_test = tanh(Z1_test)\n",
    "Z2_test = np.dot(A1_test, W2) + b2\n",
    "y_pred_test = sigmoid(Z2_test)\n",
    "\n",
    "y_pred_labels = (y_pred_test >= 0.5).astype(int)\n",
    "\n",
    "test_loss = -np.mean(y_test * np.log(y_pred_test + 1e-8) + (1 - y_test) * np.log(1 - y_pred_test + 1e-8))\n",
    "\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "Geeks4Geeks. (2025). Tanh Activation in Neural Network. https://www.geeksforgeeks.org/deep-learning/tanh-activation-in-neural-network\n",
    "\n",
    "HyperionDev. (2025). Learning Algorithms. Private repository, GitHub.\n",
    "\n",
    "HyperionDev. (2025). Neural Networks. Private repository, GitHub.\n",
    "\n",
    "Khan, M. A. (2024). The Heart of Neural Networks: Understanding Activation Functions. Medium. https://medium.com/@mohammedashfaqkhan000/the-heart-of-neural-networks-understanding-activation-functions-298b86cddd99"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

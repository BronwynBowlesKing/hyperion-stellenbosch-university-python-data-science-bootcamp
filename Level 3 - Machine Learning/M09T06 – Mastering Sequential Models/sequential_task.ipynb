{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E5GM_SllvAd"
      },
      "source": [
        "## Forecast California Housing Prices\n",
        "\n",
        "Bronwyn Bowles-King\n",
        "\n",
        "### Practical task 1: Long Short-Term Memory (LSTM) Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "E8Ap8nPskHuq"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Dataset\n",
        "from sklearn.datasets import fetch_california_housing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqlAStI5sMbL",
        "outputId": "29339712-29b7-4bd7-d7f4-86427845872c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x17ef6a16350>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set random seeds fopr NumPy and PyTorch data processing\n",
        "np.random.seed(50)\n",
        "torch.manual_seed(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ILXBVgzsDy4",
        "outputId": "f75135f0-1fee-4013-b306-a9fbb29c7ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
            "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
            "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
            "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
            "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
            "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
            "\n",
            "   Longitude  Price  \n",
            "0    -122.23  4.526  \n",
            "1    -122.22  3.585  \n",
            "2    -122.24  3.521  \n",
            "3    -122.25  3.413  \n",
            "4    -122.25  3.422  \n",
            "<class 'sklearn.utils._bunch.Bunch'>\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "data = fetch_california_housing()\n",
        "features = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "targets = pd.Series(data.target, name='Price').values.astype(float).reshape(-1, 1)\n",
        "\n",
        "# Combine features and target into one DataFrame to preview the data\n",
        "df_full = features.copy()\n",
        "df_full['Price'] = targets.flatten()\n",
        "\n",
        "print(df_full.head())\n",
        "\n",
        "print(type(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfiP0fYEuF62"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "1. What is a `Bunch`?\n",
        "2. What is the default type for `data`?\n",
        "3. What is the default type for `target`?\n",
        "\n",
        "**Answers**\n",
        "\n",
        "1. A Bunch is a container object used for scikit-learn functions to store datasets. It behaves like a Python dictionary so that there are keys and attributes to access the data.\n",
        "2. Printing the type of data above that we loaded shows that it is a Bunch. By default, the data attribute in a Bunch is a NumPy array, unless the function is called with different parameters.\n",
        "3. The target attribute is also by default a NumPy array unless different parameters are set (scikit-learn developers, 2025).\n",
        "\n",
        "### 1.1 Data cleaning and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IMX3wj0PkR6d"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, targets, seq_length):\n",
        "   X, y = [], []\n",
        "   for i in range(len(data) - seq_length):\n",
        "       X.append(data[i:i+seq_length])\n",
        "       y.append(targets[i+seq_length])\n",
        "   return np.array(X), np.array(y)\n",
        "\n",
        "features = data.data         # shape (n_samples, n_features)\n",
        "targets = data.target        # shape (n_samples,)\n",
        "\n",
        "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
        "features_normalized = scaler_features.fit_transform(features)\n",
        "\n",
        "targets = targets.reshape(-1, 1)  # 2D shape for the scaler\n",
        "scaler_targets = MinMaxScaler(feature_range=(0, 1))\n",
        "targets_normalized = scaler_targets.fit_transform(targets).flatten()\n",
        "\n",
        "seq_length = 20\n",
        "X, y = create_sequences(features_normalized, targets_normalized, seq_length)\n",
        "\n",
        "train_size = int(len(X) * 0.67)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Create TensorDataset\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Create DataLoader\n",
        "loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Anb6Nfke09"
      },
      "source": [
        "### 1.2 Create and run a baseline LSTM model with L2 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6FCav0_vGUYm"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTMModel is a Long Short-Term Memory (LSTM) neural network for sequence regression tasks.\n",
        "\n",
        "    Attributes:\n",
        "        lstm (nn.LSTM): The LSTM layer(s) that process input sequences.\n",
        "        fc (nn.Linear): Fully connected layer to produce output predictions from LSTM output.\n",
        "\n",
        "    Arguments:\n",
        "        input_size (int): Number of input features per time step.\n",
        "        hidden_size (int): Number of features in the hidden state of the LSTM.\n",
        "        output_size (int): Number of output features (e.g., 1 for regression).\n",
        "        num_layers (int): Number of recurrent LSTM layers stacked.\n",
        "\n",
        "    Forward method:\n",
        "        Processes input tensor of shape (batch_size, sequence_length, input_size) through LSTM layers,\n",
        "        applies a linear layer on the last time step's output to produce predictions of shape\n",
        "        (batch_size, output_size).\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.lstm(x) # output[:, -1, :] takes the last output in the sequence\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "C31QaPyoGUYm"
      },
      "outputs": [],
      "source": [
        "# Set baseline hyperparameters\n",
        "input_size = X.shape[2]  # Set as the same as the number of features\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "num_layers = 2\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "l2_regularization = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ZaE2DUt_GUYn"
      },
      "outputs": [],
      "source": [
        "# Instantiate model\n",
        "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Optimiser with L2 regularisation\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=l2_regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NONW-JraGUYn",
        "outputId": "110297e3-983a-4dec-b6b5-b7209aa3ae9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 | Loss: 58.5172\n",
            "Epoch 10/100 | Loss: 57.0067\n",
            "Epoch 15/100 | Loss: 56.0682\n",
            "Epoch 20/100 | Loss: 54.4676\n",
            "Epoch 25/100 | Loss: 53.3761\n",
            "Epoch 30/100 | Loss: 51.9395\n",
            "Epoch 35/100 | Loss: 51.4691\n",
            "Epoch 40/100 | Loss: 51.3245\n",
            "Epoch 45/100 | Loss: 51.1690\n",
            "Epoch 50/100 | Loss: 50.7187\n",
            "Epoch 55/100 | Loss: 50.5893\n",
            "Epoch 60/100 | Loss: 50.5240\n",
            "Epoch 65/100 | Loss: 50.8703\n",
            "Epoch 70/100 | Loss: 49.9330\n",
            "Epoch 75/100 | Loss: 50.1615\n",
            "Epoch 80/100 | Loss: 49.8897\n",
            "Epoch 85/100 | Loss: 49.7979\n",
            "Epoch 90/100 | Loss: 49.8755\n",
            "Epoch 95/100 | Loss: 49.7951\n",
            "Epoch 100/100 | Loss: 49.8809\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in loader:\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        output = model(batch_X)  # Forward pass\n",
        "        loss = criterion(output, batch_y)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(data)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV4KXK60wpl0",
        "outputId": "8cbbc60a-8b20-4a14-a439-5576edcb603d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set MSE for LSTM with L2 regularisation: 0.6148\n",
            "Average error margin: $78410\n"
          ]
        }
      ],
      "source": [
        "# Convert test data to tensors\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():  # No gradient computation here\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = y_pred_tensor.numpy()\n",
        "    y_true = y_test_tensor.numpy()\n",
        "\n",
        "# Reshape to 2D for inverse_transform\n",
        "y_pred_unscaled_1 = scaler_targets.inverse_transform(y_pred.reshape(-1, 1))\n",
        "y_true_unscaled_1 = scaler_targets.inverse_transform(y_true.reshape(-1, 1))\n",
        "\n",
        "# Calculate baseline model's MSE\n",
        "mse_1 = mean_squared_error(y_true_unscaled_1, y_pred_unscaled_1)\n",
        "print(f\"Test set MSE for LSTM with L2 regularisation: {mse_1:.4f}\")\n",
        "print(f\"Average error margin: ${np.sqrt(mse_1) * 100000:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_QzHlLoGUYn"
      },
      "source": [
        "Although random seeds were set, there is an element of randomness in the process. There can be some variation in the results at times due to factors such as non-determinism (Geeks4Geeks, 2025a). The test set result of 0.6148 above for the MSE concerns the target variable, which are median house values (in dollars) in the dataset. The typical squared error between predictions and the original data is the square root of 0.6148, which is around 0.7841. So, the average prediction is off by about $78 400. \n",
        "\n",
        "This is not a good sign as the median house prices in this dataset range from \\$45 800 to \\$500 000. The average difference between actual and predicted values is too great. We can also see the loss values tracked above for the baseline model with L2 regularisation. They remain high and much the same (~49) across all epochs, so the model is not learning over training rounds.\n",
        "\n",
        "### 1.3 Run LSTM model without L2 regularisation\n",
        "\n",
        "The LSTM model is now run without L2 regularisation, which will likely make the model perform poorly, but this needs to be checked in any case to see the effect of L2 regularisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZE_JfWeGUYo",
        "outputId": "3de84c01-0a9f-4238-9185-5e43083d7986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 | Loss: 47.6477\n",
            "Epoch 10/100 | Loss: 46.1644\n",
            "Epoch 15/100 | Loss: 45.4000\n",
            "Epoch 20/100 | Loss: 44.7677\n",
            "Epoch 25/100 | Loss: 44.2369\n",
            "Epoch 30/100 | Loss: 43.0339\n",
            "Epoch 35/100 | Loss: 42.7067\n",
            "Epoch 40/100 | Loss: 41.4925\n",
            "Epoch 45/100 | Loss: 41.1778\n",
            "Epoch 50/100 | Loss: 40.5651\n",
            "Epoch 55/100 | Loss: 39.5890\n",
            "Epoch 60/100 | Loss: 38.7754\n",
            "Epoch 65/100 | Loss: 38.0005\n",
            "Epoch 70/100 | Loss: 37.5138\n",
            "Epoch 75/100 | Loss: 36.4070\n",
            "Epoch 80/100 | Loss: 35.6938\n",
            "Epoch 85/100 | Loss: 34.6950\n",
            "Epoch 90/100 | Loss: 33.6920\n",
            "Epoch 95/100 | Loss: 32.8615\n",
            "Epoch 100/100 | Loss: 32.4240\n"
          ]
        }
      ],
      "source": [
        "# Optimiser without L2 regularisation (weight_decay=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=0)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(data)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYTAJLnvn5Xr",
        "outputId": "01ebbc86-0ed0-4764-a8e4-75dc97cf0885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set MSE for LSTM without L2 regularisation: 0.7466\n",
            "Average error margin: $86405\n"
          ]
        }
      ],
      "source": [
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = y_pred_tensor.numpy()\n",
        "    y_true = y_test_tensor.numpy()\n",
        "\n",
        "y_pred_unscaled_2 = scaler_targets.inverse_transform(y_pred.reshape(-1, 1))\n",
        "y_true_unscaled_2 = scaler_targets.inverse_transform(y_true.reshape(-1, 1))\n",
        "\n",
        "mse_2 = mean_squared_error(y_true_unscaled_2, y_pred_unscaled_2)\n",
        "print(f\"Test set MSE for LSTM without L2 regularisation: {mse_2:.4f}\")\n",
        "print(f\"Average error margin: ${np.sqrt(mse_2) * 100000:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM5qSlLtGUYo"
      },
      "source": [
        "When L2 regularisation is not included, the LSTM model performs more poorly than when it is included, with an MSE of 0.7466. The model's predictions are now off by around $86 400 on average, a wider error than the same model but with L2 regularisation that was run in section 1.2.\n",
        "\n",
        "### 1.4 Re-run LSTM model with L2 regularisation and hyperparameter tuning\n",
        "\n",
        "In one experiment, the size of the hidden layer (128) and number of epochs (200) were doubled in the training run, and the learning rate was increased to 0.01. This LSTM model, which included L2 regularisation performed much the same, with an MSE of 0.6799, which is an average error of $82 400, similar to the previous two attempts. \n",
        "\n",
        "The model was also trained for 500 epochs to give it more time to learn while keeping the other parameters the same as the baseline. However, this did not help, as the MSE is worse (0.7203). Thus, a high learning rate, larger hidden layer and increased learning epochs are not necessarily effective.\n",
        "\n",
        "When experimenting by increasing the learning rate or L2 regularisation to around 0.1 and 0.01, the model's performance also deteriorated rapidly. The error was very large in this case, and so a faster learning rate and a regularisation value that is too high does not help.\n",
        "\n",
        "I decided to test the model for different L2 values, holding all the values the same as the baseline, and the results are as below.\n",
        "\n",
        "| L2 Regularisation | MSE    |\n",
        "|-------------------|--------|\n",
        "| 0.002             | 0.6822 |\n",
        "| 0.001             | 0.6004 |\n",
        "| 0.0005            | 0.5813 |\n",
        "| 0.0001            | 0.5412 |\n",
        "| 0.00001           | 0.6315 |\n",
        "\n",
        "\n",
        "When the L2 value is decreased, the MSE improves, but only to a certain point (0.0001), where the value is too small to make any meaningful difference and has the opposite effect.\n",
        "\n",
        "After settling on an L2 value of 0.0001, and dropping the training rounds to 70, the MSE finally reached the lowest at 0.552, which is shown below. This is an error margin of about $74 300. Again, although random seeds were set, there is still randomness in the process and there can be variation in the results when rerun. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9ajlox7GUYo",
        "outputId": "7f5f6cc5-7e5f-4063-e4cc-a8237fa9250c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/70 | Loss: 41.8525\n",
            "Epoch 10/70 | Loss: 42.8302\n",
            "Epoch 15/70 | Loss: 43.3365\n",
            "Epoch 20/70 | Loss: 43.4540\n",
            "Epoch 25/70 | Loss: 43.6418\n",
            "Epoch 30/70 | Loss: 43.8008\n",
            "Epoch 35/70 | Loss: 43.7122\n",
            "Epoch 40/70 | Loss: 44.3713\n",
            "Epoch 45/70 | Loss: 44.2916\n",
            "Epoch 50/70 | Loss: 44.0099\n",
            "Epoch 55/70 | Loss: 43.8977\n",
            "Epoch 60/70 | Loss: 44.1508\n",
            "Epoch 65/70 | Loss: 43.6155\n",
            "Epoch 70/70 | Loss: 43.8619\n"
          ]
        }
      ],
      "source": [
        "input_size = X.shape[2]\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "num_layers = 2\n",
        "num_epochs = 70\n",
        "learning_rate = 0.001\n",
        "l2_regularization = 0.0001\n",
        "\n",
        "# Optimiser with L2 regularisation\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=l2_regularization)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(data)\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "yYVZmZ_RoBHX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set MSE: 0.5520\n",
            "Average error margin: $74295\n"
          ]
        }
      ],
      "source": [
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = y_pred_tensor.numpy()\n",
        "    y_true = y_test_tensor.numpy()\n",
        "\n",
        "y_pred_unscaled_3 = scaler_targets.inverse_transform(y_pred.reshape(-1, 1))\n",
        "y_true_unscaled_3 = scaler_targets.inverse_transform(y_true.reshape(-1, 1))\n",
        "\n",
        "mse_3 = mean_squared_error(y_true_unscaled_3, y_pred_unscaled_3)\n",
        "print(f\"Test set MSE: {mse_3:.4f}\")\n",
        "print(f\"Average error margin: ${np.sqrt(mse_3) * 100000:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FST2wRKEkiOz"
      },
      "source": [
        "## Practical task 2: Gated Recurrent Unit (GRU) Neural Network\n",
        "\n",
        "### 2.1 Run GRU model with L2 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KBHYfKDzGUYo"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    \"\"\"\n",
        "    GRUModel is a Gated Recurrent Unit (GRU) neural network for sequence regression.\n",
        "\n",
        "    Attributes:\n",
        "        gru (nn.GRU): The GRU layer(s) that process input sequences.\n",
        "        fc (nn.Linear): Fully connected layer to produce output predictions from GRU output.\n",
        "\n",
        "    Arguments:\n",
        "        input_size (int): Number of input features per time step.\n",
        "        hidden_size (int): Number of features in the hidden state of the GRU.\n",
        "        output_size (int): Number of output features (e.g., 1 for regression).\n",
        "        num_layers (int): Number of recurrent GRU layers stacked.\n",
        "\n",
        "    Forward method:\n",
        "        Processes input tensor of shape (batch_size, sequence_length, input_size) through GRU layers,\n",
        "        then applies a linear layer on the last time step's output to produce predictions of shape\n",
        "        (batch_size, output_size).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(input_size=input_size,\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=num_layers,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.gru(x)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MZyv10cbGUYo"
      },
      "outputs": [],
      "source": [
        "# Use the same baseline parameters so that the two models can be compared\n",
        "\n",
        "input_size = X.shape[2]\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "num_layers = 2\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "l2_regularization = 0.001\n",
        "\n",
        "model = GRUModel(input_size, hidden_size, output_size, num_layers)\n",
        "model = model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=l2_regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "kb8UKUM7GUYo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 | Loss: 0.0267\n",
            "Epoch 10/100 | Loss: 0.0263\n",
            "Epoch 15/100 | Loss: 0.0259\n",
            "Epoch 20/100 | Loss: 0.0259\n",
            "Epoch 25/100 | Loss: 0.0251\n",
            "Epoch 30/100 | Loss: 0.0240\n",
            "Epoch 35/100 | Loss: 0.0239\n",
            "Epoch 40/100 | Loss: 0.0234\n",
            "Epoch 45/100 | Loss: 0.0231\n",
            "Epoch 50/100 | Loss: 0.0230\n",
            "Epoch 55/100 | Loss: 0.0227\n",
            "Epoch 60/100 | Loss: 0.0229\n",
            "Epoch 65/100 | Loss: 0.0226\n",
            "Epoch 70/100 | Loss: 0.0226\n",
            "Epoch 75/100 | Loss: 0.0226\n",
            "Epoch 80/100 | Loss: 0.0225\n",
            "Epoch 85/100 | Loss: 0.0223\n",
            "Epoch 90/100 | Loss: 0.0223\n",
            "Epoch 95/100 | Loss: 0.0223\n",
            "Epoch 100/100 | Loss: 0.0221\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataset)\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Nr4YmF0DoHzK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set MSE for GRU with L2 regularisation: 0.6513\n",
            "Average error margin: $80703\n"
          ]
        }
      ],
      "source": [
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = y_pred_tensor.numpy()\n",
        "    y_true = y_test_tensor.numpy()\n",
        "\n",
        "y_pred_unscaled_4 = scaler_targets.inverse_transform(y_pred.reshape(-1, 1))\n",
        "y_true_unscaled_4 = scaler_targets.inverse_transform(y_true.reshape(-1, 1))\n",
        "\n",
        "mse_4 = mean_squared_error(y_true_unscaled_4, y_pred_unscaled_4)\n",
        "print(f\"Test set MSE for GRU with L2 regularisation: {mse_4:.4f}\")\n",
        "print(f\"Average error margin: ${np.sqrt(mse_4) * 100000:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INP6TsOyGUYp"
      },
      "source": [
        "Running the GRU model with L2 regularisation shows an error loss of around 0.02 over the epochs run. The MSE for this model is 0.6513, so that the margin for error from the mean is about $80 700. This model is not learning very well yet.\n",
        "\n",
        "### 2.2 Run the GRU model without L2 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "KQIE2DF7GUYp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 | Loss: 0.0212\n",
            "Epoch 10/100 | Loss: 0.0205\n",
            "Epoch 15/100 | Loss: 0.0201\n",
            "Epoch 20/100 | Loss: 0.0196\n",
            "Epoch 25/100 | Loss: 0.0189\n",
            "Epoch 30/100 | Loss: 0.0185\n",
            "Epoch 35/100 | Loss: 0.0182\n",
            "Epoch 40/100 | Loss: 0.0177\n",
            "Epoch 45/100 | Loss: 0.0175\n",
            "Epoch 50/100 | Loss: 0.0169\n",
            "Epoch 55/100 | Loss: 0.0164\n",
            "Epoch 60/100 | Loss: 0.0159\n",
            "Epoch 65/100 | Loss: 0.0153\n",
            "Epoch 70/100 | Loss: 0.0146\n",
            "Epoch 75/100 | Loss: 0.0137\n",
            "Epoch 80/100 | Loss: 0.0128\n",
            "Epoch 85/100 | Loss: 0.0121\n",
            "Epoch 90/100 | Loss: 0.0108\n",
            "Epoch 95/100 | Loss: 0.0100\n",
            "Epoch 100/100 | Loss: 0.0092\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=0)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataset)\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set MSE for GRU without L2 regularisation: 0.7343\n",
            "Average error margin: $85693\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = y_pred_tensor.numpy()\n",
        "    y_true = y_test_tensor.numpy()\n",
        "\n",
        "y_pred_unscaled_5 = scaler_targets.inverse_transform(y_pred.reshape(-1, 1))\n",
        "y_true_unscaled_5 = scaler_targets.inverse_transform(y_true.reshape(-1, 1))\n",
        "\n",
        "mse_5 = mean_squared_error(y_true_unscaled_5, y_pred_unscaled_5)\n",
        "print(f\"Test set MSE for GRU without L2 regularisation: {mse_5:.4f}\")\n",
        "print(f\"Average error margin: ${np.sqrt(mse_5) * 100000:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMUOkGelGUYp"
      },
      "source": [
        "**a. Has performance gotten worse in both models?**\n",
        "\n",
        "Yes, in both cases the LSTM and GRU models perform worse without L2 regularisation.\n",
        "\n",
        "**b. What is the importance of regularisation for optimising the efficiency of models?**\n",
        "\n",
        "Regularisation is important for optimising the efficiency of a model and how well it can generalise to unseen data. This applies to both LSTM and GRU neural networks. Regularisation adds a penalty to the model's loss function, such as the sum of weights when L1 regularisation is applied or the sum of squared weights for L2. This gets the model to avoid fitting noise or patterns from the training data that are unhelpful for the task the model was designed for. The model is less likely to 'memorise' or hold onto the wrong data, and more likely to learn the underlying trends that will allow it to generalise to new data (Lawton, 2024).\n",
        "\n",
        "Regularisation, particularly L2, keeps parameters from growing too large by reducing the size of weights. Smaller weights typically mean simpler models, which are less likely to be highly sensitive to small variations in the data, so they are less likely to overfit, and they are generally more robust and stable (Geeks4Geeks, 2025b).\n",
        "\n",
        "### 2.3 Re-run GRU model with L2 regularisation and hyperparameter tuning\n",
        "\n",
        "The GRU model responded similarly to the LSTM model when testing different hyperparameters. It was found that a learning rate or L2 regularisation value over 0.001 also made the GRU model perform poorly. More training rounds and hidden layers equally did not help the GRU model adapt. \n",
        "\n",
        "With 70 epochs and an L2 value of 0.0001, the MSE finally reached 0.5387. This is an error margin of about $73 300. Focusing on training the model further with examples where it performs most poorly may be the best next step at this stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "--EYlXCIGUYp",
        "outputId": "09129e13-a4d6-4c3b-ca38-410a34fe9fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/70 | Loss: 0.0179\n",
            "Epoch 10/70 | Loss: 0.0191\n",
            "Epoch 15/70 | Loss: 0.0194\n",
            "Epoch 20/70 | Loss: 0.0196\n",
            "Epoch 25/70 | Loss: 0.0195\n",
            "Epoch 30/70 | Loss: 0.0194\n",
            "Epoch 35/70 | Loss: 0.0196\n",
            "Epoch 40/70 | Loss: 0.0195\n",
            "Epoch 45/70 | Loss: 0.0195\n",
            "Epoch 50/70 | Loss: 0.0195\n",
            "Epoch 55/70 | Loss: 0.0194\n",
            "Epoch 60/70 | Loss: 0.0196\n",
            "Epoch 65/70 | Loss: 0.0195\n",
            "Epoch 70/70 | Loss: 0.0193\n"
          ]
        }
      ],
      "source": [
        "input_size = X.shape[2]\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "num_layers = 2\n",
        "num_epochs = 70\n",
        "learning_rate = 0.001\n",
        "l2_regularization = 0.0001\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=l2_regularization)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataset)\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Fwa3C9FwoUai"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set MSE for GRU with L2 regularisation: 0.5387\n",
            "Average error margin: $73397\n"
          ]
        }
      ],
      "source": [
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = y_pred_tensor.numpy()\n",
        "    y_true = y_test_tensor.numpy()\n",
        "\n",
        "y_pred_unscaled_6 = scaler_targets.inverse_transform(y_pred.reshape(-1, 1))\n",
        "y_true_unscaled_6 = scaler_targets.inverse_transform(y_true.reshape(-1, 1))\n",
        "\n",
        "mse_6 = mean_squared_error(y_true_unscaled_6, y_pred_unscaled_6)\n",
        "print(f\"Test set MSE for GRU with L2 regularisation: {mse_6:.4f}\")\n",
        "print(f\"Average error margin: ${np.sqrt(mse_6) * 100000:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K1HJuv4DYUc"
      },
      "source": [
        "**References**\n",
        "\n",
        "CodeSignal. (2025). Data Handling: Preparing the California Housing Dataset. https://codesignal.com/learn/courses/building-and-applying-your-neural-network-library/lessons/data-handling-preparing-the-california-housing-dataset\n",
        "\n",
        "Geeks4Geeks. (2025a). Difference between Deterministic and Non-deterministic Algorithms. https://www.geeksforgeeks.org/dsa/difference-between-deterministic-and-non-deterministic-algorithms\n",
        "\n",
        "Geeks4Geeks. (2025b). Start learning PyTorch for Beginners. https://www.geeksforgeeks.org/python/start-learning-pytorch-for-beginners\n",
        "\n",
        "Geeks4Geeks. (2025c). Regularization in Machine Learning. https://www.geeksforgeeks.org/machine-learning/regularization-in-machine-learning\n",
        "\n",
        "Geeks4Geeks. (2025d). Why Do We Need to Call zero_grad() in PyTorch? https://www.geeksforgeeks.org/deep-learning/why-do-we-need-to-call-zerograd-in-pytorch\n",
        "\n",
        "Lawton, G. (2024). Machine learning regularization explained with examples. https://www.techtarget.com/searchenterpriseai/feature/Machine-learning-regularization-explained-with-examples\n",
        "\n",
        "noplaxochia. (2024). LSTM from scratch. Medium. https://medium.com/@wangdk93/lstm-from-scratch-c8b4baf06a8b\n",
        "\n",
        "Pace, R. K., & Barry, R. (1997). Sparse spatial autoregressions. *Statistics & Probability Letters*, 33(3), 291-297.\n",
        "\n",
        "scikit-learn. (2025a). Bunch. https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html\n",
        "\n",
        "scikit-learn. (2025b). fetch_california_housing. https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

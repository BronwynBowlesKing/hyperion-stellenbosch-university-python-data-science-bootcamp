{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests and Bagged and Boosted Decision Trees \n",
    "### Predicting Survival with the Titanic Dataset\n",
    "\n",
    "Bronwyn Bowles-King"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "This Jupyter notebook explores the use of various random forest models in predicting the survival of passengers in the sinking of the Titanic, a well-known maritime disaster that occurred in 1912. The dataset contains information on demographic and ticket details, which can be contributing factors influencing survival. \n",
    "\n",
    "In this case, I am concerned with demonstrating the particular machine learning process, fine-tuning random forest, bagged and boosted models, and finding the most accurate and efficient option. The notebook includes data preparation, model training, and evaluation of model accuracy, providing an overview of the modelling process. \n",
    "\n",
    "A basic random forest is first produced with default settings and the features that contribute most to predicting whether a passenger survives or not are identified. The modelling process is then run again with different n_estimator (number of trees in the model) and max_depth (maximum depth of each tree) parameters and the accuracy of different iterations is evaluated. \n",
    "\n",
    "In the last two sections, the process is repeated with the ensemble methods known as bagged and boosted tree models and the model performance is assessed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparation steps\n",
    "\n",
    "##### 2.1 Import libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1547,
     "status": "ok",
     "timestamp": 1732266859068,
     "user": {
      "displayName": "Pierre Roodman",
      "userId": "08657076676700421712"
     },
     "user_tz": -120
    },
    "id": "mFPFeEu69axo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "titanic_df = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Check for duplicates and missing values and impute with the median\n",
    "\n",
    "The number of missing values and duplicates are checked and then the missing values in the Age column are replaced with the median age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 0\n",
      "\n",
      "Missing values by column: \n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Duplicate rows: {titanic_df.duplicated().sum()}')\n",
    "\n",
    "print(f'\\nMissing values by column: \\n{titanic_df.isnull().sum()}')  \n",
    "\n",
    "titanic_df['Age'] = titanic_df['Age'].fillna(titanic_df['Age'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create baseline random forest model \n",
    "\n",
    "#### 3.1 Isolate and prepare features and target for prediction\n",
    "\n",
    "The features for prediction are gender ('Sex'), passenger class ('Pclass'), the fare paid ('Fare'), age in years ('Age'), number of family relations on the ship ('SibSp' and 'Parch'), and where the person boarded the boat ('Embarked'). Together, these are called 'features' and they are the X variable. Some of these aspects will prove more useful in prediction than others. The target (y) that will be predicted upon is the 'Survived' column.\n",
    "\n",
    "The features and target are assigned below and categorical columns are converted to numeric or boolean data with the pandas function get_dummies. This is necessary so that the machine learning process can work better with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass          int64\n",
      "Age           float64\n",
      "SibSp           int64\n",
      "Parch           int64\n",
      "Fare          float64\n",
      "Sex_female       bool\n",
      "Sex_male         bool\n",
      "Embarked_C       bool\n",
      "Embarked_Q       bool\n",
      "Embarked_S       bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "X = titanic_df.drop(['Survived', 'Name', 'PassengerId', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "X = pd.get_dummies(X)  # Categorical columns converted to numeric or boolean\n",
    "\n",
    "print(X.dtypes)  # Check data types for ML suitability\n",
    "\n",
    "y = titanic_df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Split the data into training and test sets\n",
    "\n",
    "The data is split into a training set of 80% and a test set of 20% and the final size of the sets is printed to show the number of samples that the model will be working with in each case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 712 samples\n",
      "Test set size: 179 samples\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")  \n",
    "print(f\"Test set size: {len(X_test)} samples\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Fit the model on training data\n",
    "\n",
    "A random forest (rf) is instantiated below with default settings and then fit on the training data. A random_state is set so that the results are consistent when running the code more than once. This baseline model is then required to make predictions on the test data and the model's accuracy is printed. The baseline random forest model performs well with an accuracy of 0.804. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline random forest accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(f\"Baseline random forest accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Determine features contributing most to passenger survival and prediction accuracy\n",
    "\n",
    "The RandomForest ensemble method in sklearn allows us to calculate the importance of features in accurate prediction. The code below gets the scores for each feature. The higher the score, the more closely related to correct survival prediction the feature is. \n",
    "\n",
    "We can see that the more a passenger paid, the more likely they were to survive. This is not surprising as it is known that wealthier people in the upper passenger classes were able to board rescue boats first because their cabins were closer to the boats. \n",
    "\n",
    "Age is the second-most important factor, followed by gender, and passenger class, which is related to the fare paid. The importance of age and gender is also not surprising as women and children were given preference in boarding the rescue boats as well. The number of family relationships and place of embarkation have little influence on the prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fare: 0.262\n",
      "Age: 0.243\n",
      "Sex_male: 0.164\n",
      "Sex_female: 0.122\n",
      "Pclass: 0.086\n",
      "SibSp: 0.050\n",
      "Parch: 0.037\n",
      "Embarked_S: 0.014\n",
      "Embarked_C: 0.014\n",
      "Embarked_Q: 0.007\n"
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "indices = np.argsort(importances)[::-1]  # Sort indices\n",
    "\n",
    "for index in indices:\n",
    "    print(f\"{feature_names[index]}: {importances[index]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test and tune the random forest model parameters \n",
    "\n",
    "The number of trees in the model (n_estimators) and maximum depth (number of branches) of each tree (max_depth) are parameters that can be adjusted to improve the model outcomes. The GridSearchCV function was used here to test various n_estimators and max_depth values and find the optimum ones. \n",
    "\n",
    "Various values were checked from 2 to 9 for max_depth and 50 to 200 trees for n_estimators. Only the optimum and slightly less accurate results before and after the optimum are shown below for simplicity.\n",
    "\n",
    "The results show that the best parameters are up to 6 branches (max_depth) and 130 trees (n_estimators). With these parameters, the cross-validation (CV) accuracy is 0.829, an improvement of 0.025 over the baseline random forest model accuracy (0.804) (section 3.3).\n",
    "\n",
    "The accuracy for all the combinations of parameters tested is also displayed below to give a sense of how much the model improves with each increment in the different combinations of max_depth and n_estimators values. \n",
    "\n",
    "The same process will be followed in the next two sections to test two other models, the bagged and boosted tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 6, 'n_estimators': 130}\n",
      "Best CV accuracy: 0.829\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [120, 130, 140],\n",
    "    'max_depth': [5, 6, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest model accuracy results:\n",
      "Params: {'max_depth': 5, 'n_estimators': 120} | CV accuracy: 0.822\n",
      "Params: {'max_depth': 5, 'n_estimators': 130} | CV accuracy: 0.822\n",
      "Params: {'max_depth': 5, 'n_estimators': 140} | CV accuracy: 0.823\n",
      "Params: {'max_depth': 6, 'n_estimators': 120} | CV accuracy: 0.826\n",
      "Params: {'max_depth': 6, 'n_estimators': 130} | CV accuracy: 0.829\n",
      "Params: {'max_depth': 6, 'n_estimators': 140} | CV accuracy: 0.823\n",
      "Params: {'max_depth': 7, 'n_estimators': 120} | CV accuracy: 0.817\n",
      "Params: {'max_depth': 7, 'n_estimators': 130} | CV accuracy: 0.822\n",
      "Params: {'max_depth': 7, 'n_estimators': 140} | CV accuracy: 0.823\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "print(\"Random forest model accuracy results:\")\n",
    "for _, row in results.iterrows():\n",
    "    print(f\"Params: {row['params']} | CV accuracy: {row['mean_test_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a bagged tree model, test accuracy and fine-tune parameters\n",
    "\n",
    "In this section, an ensemble approach is taken with bootstrap aggregating, also known as bagging. It involves training multiple decision trees on different random subsets of the data and then averaging their predictions. This reduces variance and helps prevent overfitting.\n",
    "\n",
    "A similar process is followed below whereby a baseline model is created, the model accuracy is checked, and the values for n_estimators and max_depth are adjusted. With the baseline bagged model, the accuracy is 0.765, lower than the baseline random forest above (0.804), and not yet showing an improvement by using bagging for this data specifically. \n",
    "\n",
    "However, after tuning the model, a similar accuracy of 0.826 is possible with a lower max_depth of 5 and the same number of trees (130) compared to the random forest model from section 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline bagged tree accuracy: 0.765\n"
     ]
    }
   ],
   "source": [
    "# Baseline bagged tree \n",
    "bagged = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    random_state=42\n",
    ")\n",
    "bagged.fit(X_train, y_train)\n",
    "bagged_acc = bagged.score(X_test, y_test)\n",
    "print(f\"Baseline bagged tree accuracy: {bagged_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best bagging params: {'estimator__max_depth': 5, 'n_estimators': 130}\n",
      "Best bagging CV accuracy: 0.826\n"
     ]
    }
   ],
   "source": [
    "# Test parameters \n",
    "bag_param_grid = {\n",
    "    'n_estimators': [120, 130, 140],\n",
    "    'estimator__max_depth': [4, 5, 6]\n",
    "}\n",
    "\n",
    "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42)\n",
    "bag_grid = GridSearchCV(bagging, bag_param_grid, cv=5, scoring='accuracy')\n",
    "bag_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best bagging params:\", bag_grid.best_params_)\n",
    "print(f\"Best bagging CV accuracy: {bag_grid.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging results:\n",
      "Params: {'estimator__max_depth': 4, 'n_estimators': 120}, CV accuracy: 0.822\n",
      "Params: {'estimator__max_depth': 4, 'n_estimators': 130}, CV accuracy: 0.824\n",
      "Params: {'estimator__max_depth': 4, 'n_estimators': 140}, CV accuracy: 0.822\n",
      "Params: {'estimator__max_depth': 5, 'n_estimators': 120}, CV accuracy: 0.824\n",
      "Params: {'estimator__max_depth': 5, 'n_estimators': 130}, CV accuracy: 0.826\n",
      "Params: {'estimator__max_depth': 5, 'n_estimators': 140}, CV accuracy: 0.826\n",
      "Params: {'estimator__max_depth': 6, 'n_estimators': 120}, CV accuracy: 0.823\n",
      "Params: {'estimator__max_depth': 6, 'n_estimators': 130}, CV accuracy: 0.823\n",
      "Params: {'estimator__max_depth': 6, 'n_estimators': 140}, CV accuracy: 0.822\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracy results\n",
    "bag_results = pd.DataFrame(bag_grid.cv_results_)\n",
    "print(\"Bagging results:\")\n",
    "for _, row in bag_results.iterrows():\n",
    "    print(f\"Params: {row['params']}, CV accuracy: {row['mean_test_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Create a boosted tree model, test accuracy and fine-tune parameters\n",
    "\n",
    "Boosting is now used with the decision tree model as a different ensemble approach. With boosting, trees are built sequentially, and each new tree tries to correct the errors of the previous ones, giving more weight to misclassified samples to improve further prediction. Here, the Adaptive Boosting (AdaBoost) ML algorithm is applied to create stronger 'learners' from weaker ones. Models are trained until relatively low error is achieved.\n",
    "\n",
    "Despite the advantages of this model, the baseline boosted tree accuracy is 0.76, the lowest of all the models. The accuracy is also slightly higher for the optimised baseline bagged model by 0.066 (section 7) and for the optimised random forest by 0.069 (section 5) compared to the best accuracy with optimised parameters for the AdaBoost model (0.822).\n",
    " \n",
    "However, the GridSearchCV algorithm found that an accuracy of 0.822 can be achieved with only 3 branches (max_depth) and 120 trees (n_estimators). This shows that this model can achieve high accuracy on the test data with a lower number of iterations and node splits than the other two models. This can be highly useful when a tree is becoming overly complex with another method applied. \n",
    "\n",
    "Overall, the AdaBoost model is the most efficient model, and the slightly lower accuracy score can be an acceptable trade-off between model complexity and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted tree accuracy: 0.760\n"
     ]
    }
   ],
   "source": [
    "# Baseline boosted tree with AdaBoost\n",
    "boosted = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    random_state=42\n",
    ")\n",
    "boosted.fit(X_train, y_train)\n",
    "boosted_acc = boosted.score(X_test, y_test)\n",
    "print(f\"Boosted tree accuracy: {boosted_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AdaBoost params: {'estimator__max_depth': 3, 'n_estimators': 120}\n",
      "Best AdaBoost CV accuracy: 0.822\n"
     ]
    }
   ],
   "source": [
    "boost_param_grid = {\n",
    "    'n_estimators': [110, 120, 130],\n",
    "    'estimator__max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Test parameters\n",
    "boosting = AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42)\n",
    "boost_grid = GridSearchCV(boosting, boost_param_grid, cv=5, scoring='accuracy')\n",
    "boost_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best AdaBoost params:\", boost_grid.best_params_)\n",
    "print(f\"Best AdaBoost CV accuracy: {boost_grid.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting results (sorted):\n",
      "Params: {'estimator__max_depth': 2, 'n_estimators': 110}, CV accuracy: 0.810\n",
      "Params: {'estimator__max_depth': 2, 'n_estimators': 120}, CV accuracy: 0.809\n",
      "Params: {'estimator__max_depth': 2, 'n_estimators': 130}, CV accuracy: 0.808\n",
      "Params: {'estimator__max_depth': 3, 'n_estimators': 110}, CV accuracy: 0.812\n",
      "Params: {'estimator__max_depth': 3, 'n_estimators': 120}, CV accuracy: 0.822\n",
      "Params: {'estimator__max_depth': 3, 'n_estimators': 130}, CV accuracy: 0.820\n",
      "Params: {'estimator__max_depth': 4, 'n_estimators': 110}, CV accuracy: 0.799\n",
      "Params: {'estimator__max_depth': 4, 'n_estimators': 120}, CV accuracy: 0.796\n",
      "Params: {'estimator__max_depth': 4, 'n_estimators': 130}, CV accuracy: 0.791\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracy results\n",
    "boost_results = pd.DataFrame(boost_grid.cv_results_)\n",
    "\n",
    "print(\"Boosting results (sorted):\")\n",
    "for _, row in boost_results.iterrows():\n",
    "    print(f\"Params: {row['params']}, CV accuracy: {row['mean_test_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "Arora, A. (2023). Random Forest: Exploration of Bagging and Ensemble. Medium. https://medium.com/@ashisharora2204/random-forest-exploration-of-bagging-and-ensemble-a08efa5f608c\n",
    "\n",
    "Geeks for Geeks. (2025). Pandas DataFrame iterrows() Method. https://www.geeksforgeeks.org/pandas/pandas-dataframe-iterrows\n",
    "\n",
    "Geeks for Geeks. (2025). Single and Double Underscores in Python. https://www.geeksforgeeks.org/python/single-aad-double-underscores-in-python\n",
    "\n",
    "Geeks for Geeks. (2024). How to fit categorical data types for random forest classification? https://www.geeksforgeeks.org/machine-learning/how-to-fit-categorical-data-types-for-random-forest-classification\n",
    "\n",
    "Hunt, G. (2016). Titanic Dataset Investigation. https://ghunt03.github.io/DAProjects/DAP02/TitanicDatasetInvestigation.html\n",
    "\n",
    "HyperionDev. (2025). Supervised Learning â€“ Random Forests. Course materials. Private repository, GitHub.\n",
    "\n",
    "Navlani, A. (2018). AdaBoost Classifier in Python. DataCamp. https://www.datacamp.com/tutorial/adaboost-classifier-python\n",
    "\n",
    "numpy. (2024). np.argsort. https://numpy.org/doc/2.2/reference/generated/numpy.argsort.html\n",
    "\n",
    "Paul, S. (2018). Ensemble learning: Bagging, boosting, stacking and cascading classifiers in machine learning using SKLEARN and MLEXTEND libraries. Medium. https://medium.com/@saugata.paul1010/ensemble-learning-bagging-boosting-stacking-and-cascading-classifiers-in-machine-learning-9c66cb271674\n",
    "\n",
    "scikit-learn. (2024). GridSearchCV. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "scikit-learn. (2024). RandomForestClassifier. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "scikit-learn. (2024). sklearn.ensemble. https://scikit-learn.org/stable/api/sklearn.ensemble.html#module-sklearn.ensemble\n",
    "\n",
    "scikit-learn. (2024). train_test_split. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Steiger, T. (2017). Analysis of Titanic Survival Data. https://ttsteiger.github.io/projects/titanic_report.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
